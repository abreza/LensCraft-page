<section class="abstract">
  <h2>Abstract</h2>
  <p>
    Digital creators, from indie filmmakers to animation studios, face a
    persistent bottleneck: translating their creative vision into precise camera
    movements. Despite significant progress in computer vision and artificial
    intelligence, current automated filming systems struggle with a fundamental
    trade-off between mechanical execution and creative intent. Crucially,
    almost all previous works simplify the subject to a single point, ignoring
    its orientation and true volume, severely limiting spatial awareness during
    filming. LensCraft solves this problem by mimicking the expertise of a
    professional cinematographer, using a data-driven approach that combines
    cinematographic principles with the flexibility to adapt to dynamic scenes
    in real time. Our solution combines a specialized simulation framework for
    generating high-fidelity training data with an advanced neural model that is
    faithful to the script while being aware of the volume and dynamic behavior
    of the subject. Additionally, our approach allows for flexible control via
    various input modalities, including text prompts, subject trajectory and
    volume, key points, or a full camera trajectory. It offers creators a
    versatile tool to guide camera movements in line with their vision.
    Leveraging a lightweight real-time architecture, LensCraft achieves markedly
    lower computational complexity and faster inference while maintaining high
    output quality. Extensive evaluation across static and dynamic scenarios
    reveals unprecedented accuracy and coherence, setting a new benchmark for
    intelligent camera systems compared to state-of-the-art models.
  </p>
</section>
